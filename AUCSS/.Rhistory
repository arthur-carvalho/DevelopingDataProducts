# #Test the neural network on some training data
testdata <- input_NN[101:200,1:8] #Generate some squared numbers
net.results <- compute(n_net, testdata) #Run them through the neural network
#
# #Lets see what properties n_net has
ls(net.results)
#
# #Lets see the results
print(net.results$net.result)
x<-net.results$net.result
#
# #Lets display a better version of the results
y<-input_NN[101:200,9:11]
result<-apply(x, 1, which.max) - apply(y, 1, which.max)
sum(result == 0)
library("stats")
library("neuralnet")
number_of_data<- 200
input_NN  <- data.frame(matrix(ncol = 11, nrow = number_of_data))
for (i in 1:number_of_data) {
p1 <- rbeta(1, 5, 0.5)
p2 <- 1 - p1
q1<- rbeta(1, 5, 0.5)
q2 <- 1 - q1
x1 <- runif(1, 0, 100) #rnorm(1)
x2 <- runif(1, 0, 100) #rnorm(1)
y1 <- runif(1, 0, 100) #rnorm(1)
y2 <- runif(1, 0, 100) # rnorm(1)
EV1 <- p1*x1^0.88 + p2*x2^0.88
EV2 <- q1*y1^0.88 + q2*y2^0.88
if (EV1 > EV2){
input_NN[i,] <- c(p1, p2, x1, x2, q1, q2, y1, y2, 1, 0, 0)
}
else if (EV1 < EV2){
input_NN[i,] <- c(p1, p2, x1, x2, q1, q2, y1, y2, 0, 1, 0)
}
else {
input_NN[i,] <- c(p1, p2, x1, x2, q1, q2, y1, y2, 0, 0, 1)
}
}
remove(i)
colnames(input_NN) <- c("p1", "p2", "x1", "x2", "q1", "q2", "y1", "y2","Output1","Output2","Output3")
# install.packages('neuralnet')
# library("neuralnet")
#
# #Going to create a neural network to perform sqare rooting
# #Type ?neuralnet for more information on the neuralnet library
#
# #Generate 50 random numbers uniformly distributed between 0 and 100
# #And store them as a dataframe
# traininginput <-  as.data.frame(runif(50, min=0, max=100))
# trainingoutput <- sqrt(traininginput)
#
# #Column bind the data into one variable
# trainingdata <- cbind(traininginput,trainingoutput)
# colnames(trainingdata) <- c("Input","Output")
#
# #Train the neural network
# #Going to have 10 hidden layers
# #Threshold is a numeric value specifying the threshold for the partial
# #derivatives of the error function as stopping criteria.
n_net <- neuralnet(Output1+Output2+Output3~p1+p2+x1+x2+q1+q2+y1+y2,input_NN[1:number_of_data/2,], hidden=50, threshold=0.01)
print(n_net)
#
# #Plot the neural network
#plot(n_net)
# #Test the neural network on some training data
testdata <- input_NN[101:200,1:8] #Generate some squared numbers
net.results <- compute(n_net, testdata) #Run them through the neural network
#
# #Lets see what properties n_net has
ls(net.results)
#
# #Lets see the results
print(net.results$net.result)
x<-net.results$net.result
#
# #Lets display a better version of the results
y<-input_NN[101:200,9:11]
result<-apply(x, 1, which.max) - apply(y, 1, which.max)
sum(result == 0)
library("stats")
library("neuralnet")
number_of_data<- 200
input_NN  <- data.frame(matrix(ncol = 11, nrow = number_of_data))
for (i in 1:number_of_data) {
p1 <- rbeta(1, 5, 0.5)
p2 <- 1 - p1
q1<- rbeta(1, 5, 0.5)
q2 <- 1 - q1
x1 <- runif(1, 0, 100) #rnorm(1)
x2 <- runif(1, 0, 100) #rnorm(1)
y1 <- runif(1, 0, 100) #rnorm(1)
y2 <- runif(1, 0, 100) # rnorm(1)
EV1 <- p1*x1^0.88 + p2*x2^0.88
EV2 <- q1*y1^0.88 + q2*y2^0.88
if (EV1 > EV2){
input_NN[i,] <- c(p1, p2, x1, x2, q1, q2, y1, y2, 1, 0, 0)
}
else if (EV1 < EV2){
input_NN[i,] <- c(p1, p2, x1, x2, q1, q2, y1, y2, 0, 1, 0)
}
else {
input_NN[i,] <- c(p1, p2, x1, x2, q1, q2, y1, y2, 0, 0, 1)
}
}
remove(i)
colnames(input_NN) <- c("p1", "p2", "x1", "x2", "q1", "q2", "y1", "y2","Output1","Output2","Output3")
# install.packages('neuralnet')
# library("neuralnet")
#
# #Going to create a neural network to perform sqare rooting
# #Type ?neuralnet for more information on the neuralnet library
#
# #Generate 50 random numbers uniformly distributed between 0 and 100
# #And store them as a dataframe
# traininginput <-  as.data.frame(runif(50, min=0, max=100))
# trainingoutput <- sqrt(traininginput)
#
# #Column bind the data into one variable
# trainingdata <- cbind(traininginput,trainingoutput)
# colnames(trainingdata) <- c("Input","Output")
#
# #Train the neural network
# #Going to have 10 hidden layers
# #Threshold is a numeric value specifying the threshold for the partial
# #derivatives of the error function as stopping criteria.
n_net <- neuralnet(Output1+Output2+Output3~p1+p2+x1+x2+q1+q2+y1+y2,input_NN[1:number_of_data/2,], hidden=50, threshold=0.01)
print(n_net)
#
# #Plot the neural network
#plot(n_net)
# #Test the neural network on some training data
testdata <- input_NN[101:200,1:8] #Generate some squared numbers
net.results <- compute(n_net, testdata) #Run them through the neural network
#
# #Lets see what properties n_net has
ls(net.results)
#
# #Lets see the results
print(net.results$net.result)
x<-net.results$net.result
#
# #Lets display a better version of the results
y<-input_NN[101:200,9:11]
result<-apply(x, 1, which.max) - apply(y, 1, which.max)
sum(result == 0)
(38,190.5−38,000)/(10,000/sqrt(500))
(38190.5−38000)/(10000/sqrt(500))
rexp(40,0.2))
rexp(40,0.2)
tt<-rexp(40,0.2)
for (i in 1 : 100) {
tt[1] <- rexp(40,0.2)
}
for (i in 1 : 100) {
tt[i] <- rexp(40,0.2)
}
?rexp
for (i in 1 : 100) {
+          tt <- cbind(tt, rexp(40,0.2))
+     }
for (i in 1 : 100) {tt <- cbind(tt, rexp(40,0.2)) }
View(tt)
fix(tt)
View(tt)
View(tt)
for (i in 1 : 100) {tt <- cbind(tt, rexp(40,0.2)) }
tt<- rexp(40,0.2)
for (i in 1 : 99) {tt <- cbind(tt, rexp(40,0.2)) }
View(tt)
for (i in 1 : 99) {tt <- cbind(tt, rexp(100,0.2)) }
tt<- rexp(100,0.2)
for (i in 1 : 99) {tt <- cbind(tt, rexp(100,0.2)) }
View(tt)
tt<- readClipboard()
tt<- readClipboard()
tt<-as.numeric(tt)
t.test(tt)
t.test(tt, alternative = "g")
t.test(tt, alternative = "g",mu = 38000)
t.test(tt,mu = 38000)
t.test(tt,mu = 38000, conf.level = 0.05)
t.test(tt,mu = 38000, conf.level = 0.95)
0.9116/2
chisq.test(tt)
var.test(tt)
?var.test
pchisq(499 * 1469215766/1500000000, 499, lower.tail = FALSE)
prop.test(905,1508)
15/sqrt(15)
1.96*15/sqrt(15)
50    48	65	74	66	37	45	68	64
65	58	55	52	63	59	57	74	65
tt<-readClipboard()
tt<-as.numeric(readClipboard())
tt
tt<- c(50,   48, 65, 74, 66, 37, 45, 68, 64, 65, 58, 55,  52, 63, 59, 57, 74, 65)
install.packages("BSDA")
library("BSDA", lib.loc="C:/Program Files/R/R-3.1.1/library")
z.test(tt)
z.test(tt,sigma.x = 10)
z.test(tt,sigma.x = 10,alternative = "g")
tt<- c(18,16,10,13,23)
var.test(tt)
x<-as.numeric(readClipboard())
y<-as.numeric(readClipboard())
library("BSDA", lib.loc="C:/Program Files/R/R-3.1.1/library")
z.test(x,y)
z.test(x,y,sigma.x = 38,sigma.y = 38)
z.test(x,y,sigma.x = sqrt(38),sigma.y = sqrt(38))
t.test(x,y,var.equal = TRUE)
t.test(x,y,var.equal = TRUE, alternative = "g")
t.test(x,y,var.equal = false, alternative = "g")
t.test(x,y,var.equal = FALSE, alternative = "g")
t.test(x,y,var.equal = FALSE)
x<-as.numeric(readClipboard())
y<-as.numeric(readClipboard())
t.test(x,y,paired = TRUE)
t.test(x,y,paired = TRUE, alternative = "g")
t.test(x,y,paired = TRUE, alternative = "l")
sqrt(12)
9.09/sqrt(12)
-1/9.09/sqrt(12)
-1/(9.09/sqrt(12))
-1/(3.015/sqrt(12))
x<-as.numeric(readClipboard())
y<-as.numeric(readClipboard())
var.testx,y
var.test(x,y)
2*0.306
var.test(x,y,alternative = "g")
var.test(x,y,alternative = "l")
var.test(x,y)
?prop.test
smokers  <- c( 83, 90, 129, 70 )
patients <- c( 86, 93, 136, 82 )
prop.test(smokers, patients)
prop.test(c(41, 44), c(104,125))
prop.test(c(41, 44), c(104,125), correct = FALSE)
prop.test(c(41, 44), c(104,125), correct = FALSE, alternative = "l")
prop.test(c(41, 44), c(104,125), correct = FALSE, alternative = "g")
prop.test(c(180, 904), c(155,1038), correct = FALSE, alternative = "g")
prop.test(c(180, 155), c(904,1038), correct = FALSE, alternative = "g")
prop.test(c(180, 155), c(904,1038), correct = TRUE, alternative = "g")
2+2
x<-as.numeric(readClipboard())
y<-as.numeric(readClipboard())
t.test(x,y)
chisq.test(c(50,20,10,20),c(44,24,13,10))
chisq.test(c(50,20,10,20),c(44,24,13,10),correct = FALSE)
chisq.test(c(50,20,10,20),c(44,24,13,10),correct = TRUE)
x<-as.numeric(readClipboard())
y<-as.numeric(readClipboard())
cor.test(x,y)
chisq.test(x = c(102, 82, 16), c(90, 80,30))
?chisq.test
Xsq<-chisq.test(x = c(102, 82, 16), c(90, 80,30))
Xsq$observed
Xsq$expected
x <- c(A = 20, B = 15, C = 25)
x
Xsq<-chisq.test(c(102, 82, 16), c(90, 80,30))
chisq.test(c(102, 82, 16), c(90, 80,30))
chisq.test(c(102, 82, 16), c(90, 80,30),correct = FALSE)
chisq.test(c(102, 82, 16), c(90, 80,30),correct = TRUE)
chisq.test(c(102,82,16), c(90,80,30))
chisq.test(c(102,82,16), p=(0.45,0.4,0.15))
chisq.test(c(102,82,16), p = c(0.45,0.4,0.15))
x<- as.numeric(readClipboard())
y<- as.numeric(readClipboard())
y<- as.numeric(readClipboard())
reg <- lm(y ~ x)
predict(reg, 85, interval="predict")
newdata = data.frame(waiting=85)
predict(reg, newdata, interval="predict")
newdata = data.frame(x=85)
predict(reg, newdata, interval="predict")
predict(reg, newdata, interval="confidence")
x
install.packages("twitteR")
# load package twitteR
library(twitteR)
# (returns top 20 trending topics per hour for given data)
today_trends = getTrends(period = "daily", date=Sys.Date())
today_trends = getTrends(period = "daily", date=Sys.Date())
getTrends(period = "daily", date=Sys.Date())
public_tweets = publicTimeline()
x<-as.numeric(readClipboard())
y<-as.numeric(readClipboard())
var.test(x,y)
var.test(x,y, alternative = g)
var.test(x,y, alternative = "g")
0.2085*2
x<-as.numeric(readClipboard())
x<-as.numeric(readClipboard())
y<-as.numeric(readClipboard())
t.test(x,y,alternative = "g",paired = TRUE)
x<-as.numeric(readClipboard())
t.test(x,alternative = "g",mu = 0)
t.test(x,mu = 0)
x<- as.numeric(readClipboard())
hist(x)
?hist
hist(x, breaks = c(2,3,4,5,6,7,8,9,10)
)
hist(ceiling(x), breaks = c(2,3,4,5,6,7,8,9,10)
)
hist(ceiling(x), breaks = c(2,3,4,5,6,7,8,9,10,11))
plot(1:10)
x11()            # this has aliases on different OSs
plot(10:1)
plot(1:10)
x11()            # this has aliases on different OSs
plot(10:1)
x11()            # this has aliases on different OSs
plot(10:1)
x <- 1:12
dim(x)
library(gtools)
install.packages(c("boot", "class", "cluster", "codetools", "colorspace", "digest", "foreign", "KernSmooth", "lattice", "manipulate", "MASS", "Matrix", "mgcv", "nlme", "nnet", "queueing", "RColorBrewer", "Rcpp", "reshape2", "rmarkdown", "ROAuth", "rpart", "spatial", "survival", "twitteR"))
install.packages(c("ggplot2", "manipulate", "zoo"))
R.Version()
3 * (5+2)
8^(1+14)
1e2 / 9
100 %% 9
8-
log(5.2)
sqrt(9.8)
abs(-12.1)
exp(2.2)
factorial(5)
exp(1.648659)
log(9.025013)
5*4*3*2*1
pi
two_pies <- 2 * pi
x
x <- 2
y <- x * 2
z <- x^y
z - 4
z + 4
x <- 8
z + 4
z <- x^y
z + 4
(x + 3) <= 10
x <- 5
5 < 4
5 < 4
10 == 20
10 != 20
150 < exp((pi^2) / 2)
exp((pi^2) / 2) > 150
(5 < 4) & (4 < 3)
x <- 12
if (x > 10) {
x <- x / 2
}
else {
x <- x + 3
}
x
x <- 12
if (x > 10) {
x <- x / 2
}
else {
x <- x + 3
}
x
x <- 12
if (x > 10) {
x <- x/2
}
else {
x <- x + 3
}
x
x <- 12
if (x > 10) {
x <- x/2
}else {
x <- x + 3
}
x
y <- 22
if (y > 20) {
y <- y / 2
}
y
3 + 5 + 6 + 4 + 5
3 + 5 + 6 + 4 + 5
sqrt(3) + sqrt(5) + sqrt(6) + sqrt(4) + sqrt(5)
3 * 5 * 6 * 4 * 5
x <- c(3, 5, 6, 4, 5)
sum(x)
sum(sqrt(x))
prod(x)
x <- c(3, 5, 6, 4, 5)
sqrt(x)
length(sqrt(x))
x <- c(3, 5, 6, 4, 5)
x_root <- sqrt(x)
x_root[1]
x_root[2]
x_root[length(x_root)]
x_root[c(1,2,3)]
1:10
21:25
c(1:10, 21:25)
x > mean(x))
data(elecequip)
x <- as.vector(elecequip)
require(fpp)
data(elecequip)
x <- as.vector(elecequip)
install.packages("fpp")
data(elecequip)
x <- as.vector(elecequip)
require(fpp)
data(elecequip)
x <- as.vector(elecequip)
sum(x > mean(x))
selection <- x > mean(x)
sum(selection)
selection <- x > mean(x)
sum(x[selection])
x <- as.vector(elecequip)
summary(x)
qplot(x)
plot(x)
lines(x)
plot(ses(x))
# Loading libraries
library(rpart)
library(rpart.plot)
library(shiny)
#Setting working directory
setwd("C:/Users/56387aco/Google Drive/Erasmus University/Teaching/Big Data and Business Analytics/Course Documents/Sessions/OLD/ME011session01/Example1")
# Reading data
data_train <- read.csv("train.csv", stringsAsFactors = TRUE)
data_test  <- read.csv("test.csv", stringsAsFactors = TRUE)
# building a decision tree
tree <- rpart(target ~ ., data=data_train, method="class")
#plotting the tree
rpart.plot(tree, box.col=c("pink", "palegreen3")[tree$frame$yval], uniform = TRUE, varlen = 0, faclen = 0, cex = TRUE)
# Performance of the tree on the training set
results <- data.frame(Prediction = predict(tree, data_train, type = "class"), Reality = data_train[,"target"])
with(results, prop.table(table(Prediction, Reality)))
# Performance of the tree on the test set
results <- data.frame(Prediction = predict(tree, data_test, type = "class"), Reality = data_test[,"target"])
with(results, prop.table(table(Prediction, Reality)))
#Saving the decision tree
saveRDS(tree, "tree.rds")
#deployment
runApp("App-1")
# Loading libraries
library(rpart)
library(rpart.plot)
library(shiny)
#Setting working directory
setwd("C:/Users/56387aco/Google Drive/Erasmus University/Teaching/Big Data and Business Analytics/Course Documents/Sessions/OLD/ME011session01/Example1")
# Reading data
data_train <- read.csv("train.csv", stringsAsFactors = TRUE)
data_test  <- read.csv("test.csv", stringsAsFactors = TRUE)
# building a decision tree
tree <- rpart(target ~ ., data=data_train, method="class")
#plotting the tree
rpart.plot(tree, box.col=c("pink", "palegreen3")[tree$frame$yval], uniform = TRUE, varlen = 0, faclen = 0, cex = TRUE)
# Performance of the tree on the training set
results <- data.frame(Prediction = predict(tree, data_train, type = "class"), Reality = data_train[,"target"])
with(results, prop.table(table(Prediction, Reality)))
#Saving the decision tree
saveRDS(tree, "tree.rds")
runApp("App-1")
runApp("App-1")
runApp("App-1", display.mode = "showcase")
# Loading libraries
library(rpart)
library(rpart.plot)
library(shiny)
#Setting working directory
setwd("C:/Users/56387aco/Google Drive/Erasmus University/Teaching/Big Data and Business Analytics/Course Documents/Sessions/OLD/ME011session01/Example1")
# Reading data
data_train <- read.csv("train.csv", stringsAsFactors = TRUE)
data_test  <- read.csv("test.csv", stringsAsFactors = TRUE)
# building a decision tree
tree <- rpart(target ~ ., data=data_train, method="class")
#plotting the tree
rpart.plot(tree, box.col=c("pink", "palegreen3")[tree$frame$yval], uniform = TRUE, varlen = 0, faclen = 0, cex = TRUE)
# Loading libraries
library(rpart)
library(rpart.plot)
library(shiny)
#Setting working directory
setwd("C:/Users/56387aco/Google Drive/Erasmus University/Teaching/Big Data and Business Analytics/Course Documents/Sessions/OLD/ME011session01/Example1")
# Reading data
data_train <- read.csv("train.csv", stringsAsFactors = TRUE)
data_test  <- read.csv("test.csv", stringsAsFactors = TRUE)
# building a decision tree
tree <- rpart(target ~ ., data=data_train, method="class")
#plotting the tree
rpart.plot(tree, box.col=c("pink", "palegreen3")[tree$frame$yval], uniform = TRUE, varlen = 0, faclen = 0, cex = TRUE)
# Performance of the tree on the training set
results <- data.frame(Prediction = predict(tree, data_train, type = "class"), Reality = data_train[,"target"])
with(results, prop.table(table(Prediction, Reality)))
saveRDS(tree, "tree.rds")
runApp("App-1")
runApp("App-1", display.mode = "showcase")
